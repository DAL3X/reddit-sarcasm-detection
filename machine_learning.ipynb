{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from os import chdir, environ, mkdir, listdir\n",
    "from shutil import rmtree\n",
    "from os.path import exists\n",
    "import tensorflow as tf\n",
    "\n",
    "disable_gpu = False\n",
    "if (disable_gpu): \n",
    "    environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "else:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from time import time\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "def load_datasets(datasets_paths: Tuple[str]) -> Tuple[pd.DataFrame]:\n",
    "    dataframes =  list(map(lambda path: pd.read_csv(path, sep='\\t'), datasets_paths))\n",
    "    # cast comments to str\n",
    "    for dataframe in range(len(dataframes)):\n",
    "        keys = list(dataframes[dataframe].keys())\n",
    "        if ('comment' in keys): dataframes[dataframe]['comment'] = dataframes[dataframe]['comment'].astype(str)\n",
    "        if ('parent_comment' in keys): dataframes[dataframe]['parent_comment'] = dataframes[dataframe]['parent_comment'].astype(str)\n",
    "    return dataframes\n",
    "\n",
    "train, test = load_datasets(('dataset/final_train.csv', 'dataset/final_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model parameters\n",
    "class defaults:\n",
    "    # mostly optimized parameters for testing\n",
    "    use_parent_comment = False\n",
    "    use_early_stopping = True\n",
    "    meta_features = ['subreddit', 'score', 'ups', 'downs', 'time', 'day_of_week',\n",
    "        'comment_length', 'smileys', 'sarcasm_indicators', 'caps_lock', 'letter_duplication']\n",
    "    rnn_dimension = 128\n",
    "    dense_dimension = 64\n",
    "    training_epochs = 15\n",
    "    batch_size = 16\n",
    "    training_set_size = 300000   # max number comments\n",
    "    testing_set_ratio = .2       # percent of training_set_size\n",
    "    vocab_size = 4000\n",
    "    random_seed = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sarcasm_detector:\n",
    "    \"\"\"\n",
    "    wrapper class for sarcasm detection in comment text\n",
    "    https://www.kaggle.com/danofer/sarcasm\n",
    "\n",
    "    uses 'train' and 'test' as datasets\n",
    "    \n",
    "    args:\n",
    "        load:              load model from disk,        defaults to False,            bool\n",
    "        use_parent_comment use parent_comment for pred. defaults to class 'defaults', bool\n",
    "        use_early_stopping use early_stopping for pred. defaults to class 'defaults', bool\n",
    "        meta_features:     List of meta features,       defaults to class 'defaults', list of strings\n",
    "        rnn_dimension:     RNN dimensions,              defaults to class 'defaults', int\n",
    "        dense_dimension:   dense layer dimensions,      defaults to class 'defaults', int\n",
    "        training_epochs:   number of training epochs,   defaults to class 'defaults', int\n",
    "        batch_size:        batch size,                  defaults to class 'defaults', int\n",
    "        training_set_size: size of training dataset,    defaults to class 'defaults', int\n",
    "                                                         -1 for maximum size\n",
    "        testing_set_ratio: % of training_set_size,      defaults to class 'defaults', float\n",
    "        vocab_size:        size of vocabulary,          defaults to class 'defaults', int\n",
    "        random_seed:       random seed for determinism, defaults to class 'defaults', int\n",
    "        save:              save fitted model,           defaults to class 'defaults', bool\n",
    "        overwrite:         replace model on disk,       defaults to class 'defaults', bool\n",
    "        *name:             name of model,               defaults to class 'defaults', str\n",
    "\n",
    "    public functions:\n",
    "        predict(dataset: DataFrame, predict_range: Tuple):\n",
    "            - predicts label and compares to truth -\n",
    "            dataset: dataset to predict of, optional. default: test_dataset\n",
    "            predict_range: Tuple of (start, end), optional. default: (0, 50)\n",
    "                            -1 for full dataset\n",
    "\n",
    "            returns tuple of (predictions: List, evaluation)\n",
    "        \n",
    "        evaluate(dataset: DataFrame):\n",
    "            - alias of tf.model.evaluate() with default params on test_dataset -\n",
    "            dataset: dataset to predict of, optional. default: test_dataset\n",
    "\n",
    "            returns evaluation\n",
    "\n",
    "        create_model_and_fit()\n",
    "            - model creation an training included -\n",
    "\n",
    "            returns fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    # model parameters\n",
    "    _use_parent_comment: bool\n",
    "    _use_early_stopping: bool\n",
    "    _meta_features: List[int]\n",
    "    _metadata_dimension: int\n",
    "    _rnn_dimension: int\n",
    "    _dense_dimension: int\n",
    "    _training_epochs: int\n",
    "    _batch_size: int\n",
    "    _training_set_size: int\n",
    "    _testing_set_size: int\n",
    "    _testing_set_ratio: float\n",
    "    _vocab_size: int\n",
    "    _random_seed: int\n",
    "\n",
    "    # model settings\n",
    "    _save: bool\n",
    "    _name: str\n",
    "    _fitted: bool\n",
    "    _overwrite: bool\n",
    "\n",
    "    # datasets\n",
    "    train_dataset: pd.DataFrame\n",
    "    test_dataset: pd.DataFrame\n",
    "\n",
    "    # encoders\n",
    "    _comment_encoder: tf.keras.layers.TextVectorization\n",
    "    _parent_comment_encoder: tf.keras.layers.TextVectorization\n",
    "\n",
    "    # submodels, models and layers\n",
    "    _comment_model: tf.keras.Sequential\n",
    "    _parent_comment_model: tf.keras.Sequential\n",
    "    _metadata_model: tf.keras.Sequential\n",
    "    _combined: tf.keras.layers.Concatenate\n",
    "    _dense: tf.keras.layers.Dense\n",
    "    _classifier: tf.keras.layers.Dense\n",
    "    model: tf.keras.Model\n",
    "\n",
    "    # training history\n",
    "    _history: tf.keras.callbacks.History\n",
    "    _vocabulary_coverage: List[float]\n",
    "    _time: float\n",
    "\n",
    "    def __init__(self, load=False, use_parent_comment=None, use_early_stopping=None, meta_features=None, rnn_dimension=None, dense_dimension=None, training_epochs=None,\n",
    "        batch_size=None, training_set_size=None, testing_set_ratio=None, vocab_size=None, random_seed=None, save=False, overwrite=False, name=None) -> None:\n",
    "        assert (name != None), \"model name is required! use: name=<name>\"\n",
    "        if (load == True):\n",
    "            # model gets loaded from file\n",
    "            assert (exists(f'models/{name}')), 'requested model does not exist!'\n",
    "            self.model   = tf.keras.models.load_model(f'models/{name}')\n",
    "            self._name   = name\n",
    "            self._fitted = True\n",
    "            self.train_dataset, self.test_dataset = load_datasets((f'models/{name}/train_dataset.csv', f'models/{name}/test_dataset.csv'))\n",
    "            features = list(self.train_dataset.keys())[2:]\n",
    "            if ('parent_comment' in features):\n",
    "                self._use_parent_comment = True\n",
    "                self._meta_features = features[1:]\n",
    "            else:\n",
    "                self._use_parent_comment = False\n",
    "                self._meta_features = features\n",
    "            self._metadata_dimension = len(self._meta_features)\n",
    "        else:\n",
    "            # model gets created and trained\n",
    "            training_set_size_maximum = min(min(train['label'].value_counts()), min(test['label'].value_counts())*5)*2\n",
    "            training_set_size_maximum -= training_set_size_maximum % 100\n",
    "\n",
    "            if (training_set_size == -1): training_set_size = training_set_size_maximum\n",
    "\n",
    "            self._use_parent_comment = use_parent_comment if (use_parent_comment != None) else defaults.use_parent_comment\n",
    "            self._use_early_stopping = use_early_stopping if (use_early_stopping != None) else defaults.use_early_stopping\n",
    "            self._meta_features      = meta_features      if (meta_features != None)      else defaults.meta_features\n",
    "            self._rnn_dimension      = rnn_dimension      if (rnn_dimension != None)      else defaults.rnn_dimension\n",
    "            self._dense_dimension    = dense_dimension    if (dense_dimension != None)    else defaults.dense_dimension\n",
    "            self._training_epochs    = training_epochs    if (training_epochs != None)    else defaults.training_epochs\n",
    "            self._batch_size         = batch_size         if (batch_size != None)         else defaults.batch_size\n",
    "            self._training_set_size  = training_set_size  if (training_set_size != None)  else defaults.training_set_size\n",
    "            self._vocab_size         = vocab_size         if (vocab_size != None)         else defaults.vocab_size\n",
    "            self._random_seed        = random_seed        if (random_seed != None)        else defaults.random_seed\n",
    "            self._testing_set_ratio  = testing_set_ratio  if (testing_set_ratio != None)  else defaults.testing_set_ratio\n",
    "            self._save               = save               if (save != None)               else False\n",
    "            self._overwrite          = overwrite          if (overwrite != None)          else False\n",
    "            self._name               = name               if (name != None)               else None    # redundant but ocd is killing me\n",
    "            \n",
    "            self._testing_set_size   = int(self._testing_set_ratio * self._training_set_size)\n",
    "            self._metadata_dimension = len(self._meta_features)\n",
    "            self._fitted             = False\n",
    "\n",
    "            # make sure parameters will work\n",
    "            assert ((self._training_set_size % 2) == 0) and ((self._testing_set_size % 2) == 0) and (self._testing_set_size <= train.shape[0]), 'invalid training_set_size!'\n",
    "            assert not (self._overwrite == False and self._save == True and exists(f'models/{self._name}')), \"save is set True but model with that name already exists!\\nuse overwrite=True or delete model before creating instance.\"\n",
    "            if (self._save and not exists('models/' + self._name)): mkdir(f'models/{self._name}')\n",
    "            \n",
    "            # warning\n",
    "            if ((self._metadata_dimension % 2) != 0): print('length of meta_features *should* be an even number!')\n",
    "        print(f'sarcasm_detector \\'{self._name}\\' initialized!')\n",
    "\n",
    "    def _evaluate_vocab(self, text_encoder: tf.keras.layers.TextVectorization, comments: pd.Series) -> float:\n",
    "        # evaluate the encoder with given comment data\n",
    "        vocab = np.array(text_encoder.get_vocabulary())\n",
    "        encoded_comment = vocab[text_encoder(comments).numpy()]\n",
    "        missrate = 1 - (np.sum(encoded_comment == '[UNK]') / np.count_nonzero(encoded_comment))\n",
    "        return missrate\n",
    "\n",
    "    def predict(self, dataset=None, predict_range=(0, 50)) -> Tuple:\n",
    "        # predicts on dataset and evaluates binary decission\n",
    "        assert(self._fitted), \"model needs to be fitted first!\"  \n",
    "        dataset = dataset if (type(dataset) == pd.core.frame.DataFrame) else self.test_dataset\n",
    "        dataset = dataset[list(self.train_dataset.keys())]\n",
    "        predict_range = predict_range if (predict_range != -1) else (0, dataset.shape[0])\n",
    "        # uses only labels the model was fitted for, prevents tensor shape mismatch\n",
    "        inputs = [dataset['comment'][predict_range[0]:predict_range[1]]]\n",
    "        if (self._use_parent_comment): inputs.append(dataset['parent_comment'][predict_range[0]:predict_range[1]])\n",
    "        if (self._metadata_dimension > 0): inputs.append(dataset[self._meta_features][predict_range[0]:predict_range[1]])\n",
    "        predictions = list(self.model.predict([inputs]).flatten())\n",
    "        truths = list(dataset['label'][predict_range[0]:predict_range[1]])\n",
    "        evaluation = sum(map(lambda x: 1 if (int(round(predictions[x], 0)) == truths[x]) else 0, range(predict_range[1] - predict_range[0]))) / (predict_range[1] - predict_range[0])\n",
    "        print_arr = np.array(list(map(lambda x: [truths[x], int(round(predictions[x], 0))], range(predict_range[1] - predict_range[0]))))\n",
    "        return (print_arr, evaluation)\n",
    "\n",
    "    def evaluate(self, dataset=None):\n",
    "        # evaluates model\n",
    "        assert(self._fitted), \"model needs to be fitted first!\"\n",
    "        dataset = dataset if (type(dataset) == pd.core.frame.DataFrame) else self.test_dataset    \n",
    "        dataset = dataset[list(self.train_dataset.keys())]\n",
    "        # uses only labels the model was fitted for, prevents tensor shape mismatch\n",
    "        inputs = [dataset['comment']]\n",
    "        if (self._use_parent_comment): inputs.append(dataset['parent_comment'])\n",
    "        if (self._metadata_dimension > 0): inputs.append(dataset[self._meta_features])\n",
    "        return self.model.evaluate(x=inputs, y=dataset['label'])\n",
    "\n",
    "    def create_model_and_fit(self) -> tf.keras.Model:\n",
    "        starting_time = time()\n",
    "        # prepare scrambled datasets and balance label 1/0\n",
    "        # uses 'train' and 'test' datasets\n",
    "        print(f'{self._name}: 1/7 prepare scrambled datasets and balance label 1/0 ...')\n",
    "        train_datasets_0_1 = [train[train.label == 0].sample(int(self._training_set_size/2), random_state=self._random_seed), \n",
    "            train[train.label == 1].sample(int(self._training_set_size/2), random_state=self._random_seed)]\n",
    "        test_datasets_0_1 = [test[test.label == 0].sample(int(self._testing_set_size/2), random_state=self._random_seed), \n",
    "            test[test.label == 1].sample(int(self._testing_set_size/2), random_state=self._random_seed)]\n",
    "\n",
    "        _train_dataset = train_datasets_0_1[0].append(train_datasets_0_1[1]).sample(frac=1)\n",
    "        _test_dataset = test_datasets_0_1[0].append(test_datasets_0_1[1]).sample(frac=1)\n",
    "\n",
    "        features = ['label', 'comment', 'parent_comment'] if self._use_parent_comment else ['label', 'comment']\n",
    "        self.train_dataset = _train_dataset[features + self._meta_features]\n",
    "        self.test_dataset = _test_dataset[features + self._meta_features]\n",
    "\n",
    "        # create vocabulary of the 'vocab_size' most used words\n",
    "        print(f'{self._name}: 2/7 create vocabulary of the {self._vocab_size} most used words ...')\n",
    "        rd.seed(self._random_seed)\n",
    "        self._comment_encoder = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=self._vocab_size,\n",
    "            name=self._name + '_comment_encoder')\n",
    "        self._comment_encoder.adapt(self.train_dataset['comment'])\n",
    "        if (self._use_parent_comment):\n",
    "            rd.seed(self._random_seed)\n",
    "            self._parent_comment_encoder = tf.keras.layers.TextVectorization(\n",
    "                max_tokens=self._vocab_size,\n",
    "                name=self._name + '_parent_comment_encoder')\n",
    "            self._parent_comment_encoder.adapt(self.train_dataset['parent_comment'])\n",
    "\n",
    "        # evaluate vocabulary coverage\n",
    "        print(f'{self._name}: 3/7 evaluate vocabulary coverage ...')\n",
    "        self._vocabulary_coverage = [self._evaluate_vocab(self._comment_encoder, self.train_dataset['comment'].head(10000))]\n",
    "        if (self._use_parent_comment):\n",
    "            self._vocabulary_coverage.append(self._evaluate_vocab(self._parent_comment_encoder, self.train_dataset['parent_comment'].head(10000)))\n",
    "\n",
    "        # create models\n",
    "        print(f'{self._name}: 4/7 create models ...')\n",
    "        self._comment_model = tf.keras.Sequential([\n",
    "            self._comment_encoder,\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=len(self._comment_encoder.get_vocabulary()),\n",
    "                output_dim=self._rnn_dimension,\n",
    "                #mask_zero=True,\n",
    "                name=f'{self._name}_comment_embedding'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self._rnn_dimension, name=f'{self._name}_comment_LSTM')),\n",
    "            tf.keras.layers.Dropout(0.2)\n",
    "        ])\n",
    "        if (self._use_parent_comment):\n",
    "            self._parent_comment_model = tf.keras.Sequential([\n",
    "                self._parent_comment_encoder,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(self._parent_comment_encoder.get_vocabulary()),\n",
    "                    output_dim=self._rnn_dimension,\n",
    "                    #mask_zero=True,\n",
    "                    name=f'{self._name}_parent_comment_embedding'),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self._rnn_dimension, name=f'{self._name}_parent_comment_LSTM')),\n",
    "                tf.keras.layers.Dropout(0.2)\n",
    "            ])\n",
    "        if (self._metadata_dimension > 0):\n",
    "            self._metadata_model = tf.keras.Sequential([tf.keras.layers.Input(shape=(self._metadata_dimension,), name=f'{self._name}_metadata_input')])\n",
    "\n",
    "        # concatenate metadata with comment text and compile final model\n",
    "        print(f'{self._name}: 5/7 concatenate metadata with comment text and compile final model ...')\n",
    "        outputs = [self._comment_model.output]\n",
    "        inputs = [self._comment_model.input]\n",
    "        if (self._use_parent_comment): \n",
    "            outputs.append(self._parent_comment_model.output)\n",
    "            inputs.append(self._parent_comment_model.input)\n",
    "        if (self._metadata_dimension > 0): \n",
    "            outputs.append(self._metadata_model.output)\n",
    "            inputs.append(self._metadata_model.input)\n",
    "        if (len(outputs) > 1): \n",
    "            self._combined = tf.keras.layers.Concatenate(axis=1, name=f'{self._name}_concat')(outputs)\n",
    "            self._dense = tf.keras.layers.Dense(self._dense_dimension, activation = 'relu', name=f'{self._name}_dense') (self._combined)\n",
    "        else:\n",
    "            self._dense = tf.keras.layers.Dense(self._dense_dimension, activation = 'relu', name=f'{self._name}_dense') (self._comment_model.output)\n",
    "        self._classifier = tf.keras.layers.Dense(1, activation = 'sigmoid', name=f'{self._name}_classifier') (self._dense)\n",
    "        self.model = tf.keras.Model(inputs, self._classifier)\n",
    "        self.model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "\n",
    "        # fit model\n",
    "        print(f'{self._name}: 6/7 fit model ...')\n",
    "        input_x = [self.train_dataset['comment']]\n",
    "        val_x = [self.test_dataset['comment']]\n",
    "        callbacks = []\n",
    "        if (self._use_parent_comment): \n",
    "            input_x.append(self.train_dataset['parent_comment'])\n",
    "            val_x.append(self.test_dataset['parent_comment'])\n",
    "        if (self._metadata_dimension > 0): \n",
    "            input_x.append(self.train_dataset[self._meta_features])\n",
    "            val_x.append(self.test_dataset[self._meta_features])\n",
    "        if (self._save): callbacks.append(tf.keras.callbacks.CSVLogger(f'models/{self._name}/history.log'))\n",
    "        if (self._use_early_stopping): callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True))\n",
    "        self._history = self.model.fit(\n",
    "            x=input_x, y=self.train_dataset['label'],\n",
    "            validation_data=(val_x, self.test_dataset['label']),\n",
    "            epochs=self._training_epochs, batch_size=self._batch_size,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        self._fitted=True\n",
    "        self._time = (time() - starting_time) / 60\n",
    "\n",
    "        # save model output\n",
    "        if (self._save):\n",
    "            print(f'{self._name}: 7/7 save model output ...')\n",
    "            \n",
    "            # model data\n",
    "            tf.keras.models.save_model(self.model, f'models/{self._name}')\n",
    "\n",
    "            # datasets\n",
    "            self.train_dataset.to_csv(path_or_buf=f'models/{self._name}/train_dataset.csv', sep='\\t', index=False)\n",
    "            self.test_dataset.to_csv(path_or_buf=f'models/{self._name}/test_dataset.csv', sep='\\t', index=False)\n",
    "\n",
    "            # training log\n",
    "            loss = self._history.history['loss']\n",
    "            accuracy = self._history.history['binary_accuracy']\n",
    "            val_loss = self._history.history['val_loss']\n",
    "            val_accuracy = self._history.history['val_binary_accuracy']\n",
    "            with open('models/' + self._name + '/train_log.txt', 'w') as log_file:\n",
    "                log_file.write('model name: %s\\n' % self._name)\n",
    "                log_file.write('  parameters:\\n')\n",
    "                log_file.write('    use_parent_comment: %s\\n' % str(self._use_parent_comment))\n",
    "                log_file.write('    input_features: %s\\n' % self._meta_features)\n",
    "                log_file.write('    metadata_dimension: %d\\n' % self._metadata_dimension)\n",
    "                log_file.write('    rnn_dimension: %d\\n' % self._rnn_dimension)\n",
    "                log_file.write('    dense_dimension: %d\\n' % self._dense_dimension)\n",
    "                log_file.write('    training_epochs: %d\\n' % self._training_epochs)\n",
    "                log_file.write('    batch_size: %d\\n' % self._batch_size)\n",
    "                log_file.write('    training_set_size: %d\\n' % self._training_set_size)\n",
    "                log_file.write('    testing_set_size: %d\\n' % self._testing_set_size)\n",
    "                log_file.write('    vocab_size: %d\\n' % self._vocab_size)\n",
    "                log_file.write('    random_seed: %d\\n' % self._random_seed)\n",
    "                log_file.write('  history:\\n')\n",
    "                log_file.write('    comment encoder vocab coverage %s:\\n' % self._vocabulary_coverage)\n",
    "                log_file.write('    elapsed time: %6.2fmin\\n' % self._time)\n",
    "                for epoch in range(len(loss)):\n",
    "                    log_file.write('    epoch: %2d loss: %10.5f accuracy: %10.5f validation-loss: %10.5f validation-accuracy %10.5f\\n' % (epoch+1, loss[epoch], accuracy[epoch], val_loss[epoch], val_accuracy[epoch]))\n",
    "        else: print(f'{self._name}: 7/7 skip saving model output ...')\n",
    "        print('done.')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of meta_features *should* be an even number!\n",
      "sarcasm_detector 'tester' initialized!\n",
      "tester: 1/7 prepare scrambled datasets and balance label 1/0 ...\n",
      "tester: 2/7 create vocabulary of the 4000 most used words ...\n",
      "tester: 3/7 evaluate vocabulary coverage ...\n",
      "tester: 4/7 create models ...\n",
      "tester: 5/7 concatenate metadata with comment text and compile final model ...\n",
      "tester: 6/7 fit model ...\n",
      "Epoch 1/2\n",
      "63/63 [==============================] - 3s 20ms/step - loss: 670.9819 - binary_accuracy: 0.4880 - val_loss: 15.1627 - val_binary_accuracy: 0.4800\n",
      "Epoch 2/2\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 17.9080 - binary_accuracy: 0.5320 - val_loss: 60.0858 - val_binary_accuracy: 0.5000\n",
      "tester: 7/7 save model output ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/tester\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/tester\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 60.0868 - binary_accuracy: 0.5000\n",
      "sarcasm_detector 'tester' initialized!\n",
      "7/7 [==============================] - 1s 33ms/step - loss: 58.6942 - binary_accuracy: 0.5000\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "#test class\n",
    "tester = sarcasm_detector(name='tester',training_epochs=2, training_set_size=1000, save=True, overwrite=True)\n",
    "tester.create_model_and_fit()\n",
    "tester.predict()\n",
    "tester.evaluate()\n",
    "del tester\n",
    "tester = sarcasm_detector(name='tester', load=True)\n",
    "tester.predict()\n",
    "tester.evaluate()\n",
    "del tester\n",
    "rmtree('models/tester')\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models an fit them\n",
    "# doesn't keep instance but saves model to disk\n",
    "sarcasm_detector(name='default', save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(name='batch_size_8', batch_size=8, save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(name='only_comment', meta_features=[], save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(name='rnn_dimensions_64', rnn_dimension=64, save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(name='vocab_size_2000', vocab_size=2000, save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(name='with_parent_comment', use_parent_comment=True, meta_features=['subreddit', 'score', 'ups', 'downs', 'time', 'day_of_week', 'comment_length',\n",
    "        'parent_comment_length', 'smileys', 'smileys_parent',\n",
    "        'sarcasm_indicators', 'sarcasm_indicators_parent', 'caps_lock',\n",
    "        'caps_lock_parent', 'letter_duplication', 'letter_duplication_parent'], save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(rnn_dimension=256, name='rnn_dimensions_256', save=True, overwrite=True).create_model_and_fit()\n",
    "\n",
    "sarcasm_detector(name='full_with_parent_comment', training_set_size=-1, use_parent_comment=True, meta_features=['subreddit', 'score', 'ups', 'downs', 'time', 'day_of_week', 'comment_length',\n",
    "        'parent_comment_length', 'smileys', 'smileys_parent',\n",
    "        'sarcasm_indicators', 'sarcasm_indicators_parent', 'caps_lock',\n",
    "        'caps_lock_parent', 'letter_duplication', 'letter_duplication_parent'], save=True, overwrite=True).create_model_and_fit()\n",
    "sarcasm_detector(name='full_default', training_set_size=-1, save=True, overwrite=True).create_model_and_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions of model for later visualizations\n",
    "model_list = listdir('models')\n",
    "\n",
    "for model_name in model_list:\n",
    "    model_class = sarcasm_detector(load=True, name=model_name)\n",
    "    predictions, evaluation = model_class.predict(predict_range=-1)\n",
    "    np.save(f'models/{model_name}/predictions.npy', predictions)\n",
    "    np.save(f'models/{model_name}/evaluation.npy', evaluation)\n",
    "    del model_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
